# ğŸš€ ML-Docker-Orchestrator with Full MLops Pipeline

A containerized machine learning deployment pipeline using **PyTorch**, **Docker Compose**, **Kubernetes**, **Terraform**, **Ansible**, **Apache Kafka**, and **Snowflake**.
https://codecov.io/gh/Trojan3877/ML-Docker-Orchestrator/branch/main/graph/badge.svg
![Coverage](https://codecov.io/gh/Trojan3877/<REPO>/branch/main/graph/badge.svg)


![Docker](https://img.shields.io/badge/Containerized-Docker-informational)
![Kubernetes](https://img.shields.io/badge/Orchestrator-Kubernetes-blue)
![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-success)
![ML-Framework](https://img.shields.io/badge/Framework-PyTorch-red)
![IaC](https://img.shields.io/badge/Infrastructure-Terraform-purple)
![Provisioning](https://img.shields.io/badge/Provisioning-Ansible-yellow)
![Streaming](https://img.shields.io/badge/Streaming-Apache_Kafka-orange)
![DataWarehouse](https://img.shields.io/badge/Data-Snowflake-lightblue)
![License](https://img.shields.io/badge/License-MIT-green)

---

## ğŸ§  Architecture Flowchart

![image](https://github.com/user-attachments/assets/d9044d80-a9d7-42f7-8957-19bd2e9e9e77)



---
## ğŸš€ Quickstart â€” Run the Full MLOps Pipeline Locally

Get the complete ML Docker orchestration and MLOps pipeline running on your machine in just a few minutes.

This Quickstart will help you:
- Clone the repo
- Configure environment variables
- Build and run all services with Docker
- Verify the ML API is live
- Run basic tests

---

### ğŸ“Œ Prerequisites

Make sure you have the following installed:

- **Git**
- **Docker** (v20+)
- **Docker Compose** (v2+)
- **Python 3.8+** (for local scripts/tests)
- **pip**

Optional (for Kubernetes mode):
- **kubectl**
- **minikube** or **kind**

Check versions:

```bash
docker --version
docker compose version
python --version
Clone the Repository

Bash
git clone https://github.com/Trojan3877/ML-Docker-Orchestrator-with-full-MLops-pipeline.git
cd ML-Docker-Orchestrator-with-full-MLops-pipeline
âš™ï¸ 2ï¸âƒ£ Configure Environment
Create your environment file:
Bash
cp .env.example .env
Edit .env and adjust as needed:
Env
API_PORT=8000
MODEL_NAME=baseline_model
ENV=dev
Build & Launch with Docker Compose
Spin up the full local stack:
Bash
docker compose up --build
Verify the ML API
Once running, open another terminal and check:
Bash
curl http://localhost:8000/health
Expected output:
Json
{"status": "ok"}
You can also open in browser:

http://localhost:8000/docs
to see the FastAPI Swagger UI.
Run Tests
Run unit/integration tests locally:
Bash
pytest
Or inside the API container:
Bash
docker compose exec api pytest
 Check Running Services

Bash
docker compose ps
View logs:
Bash
docker compose logs -f
Stop the Stack
Bash
docker compose down
To remove volumes too:
Bash
docker compose down -v
## ğŸ“‚ Project Structure

```plaintext
ML-Docker-Orchestrator/
â”œâ”€â”€ model/train.py
â”œâ”€â”€ app/main.py
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ k8s/deployment.yaml
â”œâ”€â”€ terraform/main.tf
â”œâ”€â”€ ansible/provision.yml
â”œâ”€â”€ snowflake/connect_snowflake.py
â”œâ”€â”€ apache/stream_data.py
â”œâ”€â”€ tests/test_api.py
â”œâ”€â”€ notebooks/eda_snowflake_data.ipynb
â”œâ”€â”€ .github/workflows/ci.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ README.md
â””â”€â”€ visual_flowchart.png
